{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning \n",
    "\n",
    "Much of modern machine is reducing learning to numerical optimization, in supervised learning this is about minimizing training error as defined by our loss function and \n",
    "\n",
    "Learning is about making some kind of prediction on data you haven't seen before. \n",
    "\n",
    "In reinforcement learning we are trying to teach an agent to make optimal decisions given some environment. \n",
    "\n",
    "![flow_diagram](../assets/flow_diagram.png)\n",
    "\n",
    "In reinforcement learning problems we define our interactions with: \n",
    "1. Agent\n",
    "2. Environment (state a time t, $s_t$)\n",
    "3. Action (a set of possible actions given the state, can be discrete or continuous, denoted as $a_t$ in q-learning literature but usually $u_t$ in policy gradient literature)\n",
    "3. Reward ($r$)\n",
    "4. Value function (what we use to evaluate our options)\n",
    "5. Policy ($\\pi$)\n",
    "\n",
    "### Where is this used\n",
    "\n",
    "1. Control theory\n",
    "2. Swarm intelligence\n",
    "3. Game theory\n",
    "4. Multi-agent systems\n",
    "5. simulation-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI\n",
    "\n",
    "\n",
    "### Environments\n",
    "Go to [OpenAI environments](https://gym.openai.com/envs/#classic_control) to see what environments exist to test algorithms. Each environment is defined by a very small API. \n",
    "\n",
    "1. Set the chosen environment with `env = gym.make('envName)`\n",
    "2. `env.reset()` will start the environment and return the initial observations \n",
    "3. `env.action_space()` and `env.observation_space()` describe the set of valid actions and observations. \n",
    "4. Use whatever policy function you want to decide the correct action. \n",
    "5. Use `step()` to take your chosen action and return the new state of the environment.\n",
    "\n",
    "The environment's .step() function returns an:\n",
    "1. Observation (Object)- Object representing environment\n",
    "2. Reward (Float)- Reward from previous action\n",
    "3. Done (Boolean) - Whether to reset the environment again. \n",
    "4. Info (dict) - Diagnostic information for debugging. \n",
    "\n",
    "```python\n",
    "import gym \n",
    "import random\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "action = 1\n",
    "state = env.reset()\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "new_state, reward, done, _ = env.step(action)\n",
    "```\n",
    "\n",
    "### Additional features from OpenAI\n",
    "\n",
    "We can render and save a video of the environment ot see what the agent is doing. \n",
    "\n",
    "```python\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = Monitor(env, directory= '/tmp/cartpole-v0/', force=True)\n",
    "\n",
    "env.render()\n",
    "\n",
    "env.close()\n",
    "Monitor.close(env)\n",
    "\n",
    "```\n",
    "\n",
    "**Note:** For my desktop environment, the env window cannot be closed and I have to use `xkill` in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#random search\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "episodes = []\n",
    "\n",
    "\n",
    "def run_episode(env, parameters):\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    for t in range(200):\n",
    "        env.render()\n",
    "        action = 0 if np.matmul(parameters, observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            #print(\"After {} timesteps, reward: {}\".format(t+1, totalreward))\n",
    "            break\n",
    "    return totalreward\n",
    "\n",
    "def random_search(iterations):\n",
    "    bestparams = None  \n",
    "    bestreward = 0  \n",
    "    for i in range(iterations):  \n",
    "        parameters = np.random.rand(4) * 2 - 1\n",
    "        reward = run_episode(env,parameters)\n",
    "        if reward > bestreward:\n",
    "            bestreward = reward\n",
    "            bestparams = parameters\n",
    "            # considered solved if the agent lasts 200 timesteps\n",
    "            if reward == 200:                \n",
    "                print(bestparams)\n",
    "                episodes.append(i+1)\n",
    "                break\n",
    "                \n",
    "for _ in range(10):\n",
    "    random_search(100)\n",
    "\n",
    "# print(episodes)\n",
    "length = np.arange(len(episodes))\n",
    "\n",
    "plt.title('Episodes for each trial')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Episodes to train')\n",
    "plt.bar(length, episodes)\n",
    "plt.xticks(length)\n",
    "plt.show()\n",
    "\n",
    "plt.title('Histogram of episodes')\n",
    "plt.xlabel('Episodes to train')\n",
    "plt.ylabel('Frequency')\n",
    "plt.hist(episodes, normed=True, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to define a policy\n",
    "\n",
    "How we define our policy is the crux of reinforcemnt learning. \n",
    "\n",
    "1. Q-learning\n",
    "2. Policy Iteration\n",
    "3. Value Iteration\n",
    "4. Actor-Critic Methods\n",
    "5. Hierarchical Reinforcement Learning\n",
    "6. Evolutionary Strategies\n",
    "\n",
    "#### Bellman Equation\n",
    "$Q(s, a) = r + \\gamma(max(Q(s',a'))$\n",
    "\n",
    "The Bellman equation states that the long-term expected reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.5185\n",
      "Final Q-Table Values\n",
      "[[  3.78365408e-01   3.19672444e-03   4.66651309e-03   4.21110963e-03]\n",
      " [  9.01604357e-04   8.56360196e-04   1.27049922e-03   2.26436655e-01]\n",
      " [  1.88839123e-03   5.89853960e-04   9.04723579e-04   2.77396130e-01]\n",
      " [  1.18045713e-04   1.41056641e-04   1.39034575e-04   1.11945727e-01]\n",
      " [  5.82102532e-01   1.11049658e-03   2.41249062e-04   1.95105083e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.84056165e-04   2.88133738e-09   2.35770737e-02   1.75288909e-05]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  9.48150651e-04   4.98761853e-03   2.13492930e-04   7.10224453e-01]\n",
      " [  7.39328000e-04   3.88701865e-01   0.00000000e+00   5.17810257e-03]\n",
      " [  8.08210010e-01   1.61618763e-04   0.00000000e+00   5.98860968e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  9.19175543e-03   1.58622348e-03   8.26624631e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   9.71278426e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "lr = .8\n",
    "y = .95\n",
    "num_episodes = 2000\n",
    "\n",
    "reward_list = []\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    reward_all = 0\n",
    "    done = False\n",
    "    j = 0\n",
    "    \n",
    "    while j < 99:\n",
    "        j+= 1\n",
    "        action = np.argmax(Q[state,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))        \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        Q[state, action] = Q[state, action] + lr*(reward + y*np.max(Q[new_state,:]) - Q[state, action])\n",
    "        reward_all += reward\n",
    "        state = new_state\n",
    "        if done == True:\n",
    "            break\n",
    "    reward_list.append(reward_all)\n",
    "    \n",
    "    \n",
    "print(\"Score over time: {}\".format(sum(reward_list)/num_episodes))\n",
    " \n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "observations = tf.placeholder(shape=[1, env.observation_space.n], dtype=tf.float32)\n",
    "weights = tf.Variable(tf.random_uniform([16,4], 0, 0.01))\n",
    "Qout = tf.matmul(observations, weights)\n",
    "predict = tf.argmax(Qout, 1)\n",
    "\n",
    "nextQ = tf.placeholder(shape=[1, 4], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "update_model = trainer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "gamma = .99\n",
    "epsilon = 0.1\n",
    "num_episodes = 2000\n",
    "\n",
    "j_list = []\n",
    "r_list = []\n",
    " \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        reward_all = 0\n",
    "        done = False\n",
    "        j = 0\n",
    "\n",
    "        while j < 99:\n",
    "            j+= 1\n",
    "            \n",
    "            a, allQ = sess.run([predict, Qout], feed_dict={observations:np.identity(16)[s:s+1]})\n",
    "            if np.random.rand(1) < epsilon:\n",
    "                a[0] = env.action_space.sample()\n",
    "\n",
    "            s1, reward, done, _ = env.step(a[0])\n",
    "            Q1 = sess.run(Qout, feed_dict={observations: np.identity(16)[s1:s1+1]}) \n",
    "\n",
    "            \n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "         \n",
    "            targetQ[0, a[0]] = reward + gamma*maxQ1\n",
    "             \n",
    "            _, W1 = sess.run([update_model, weights], feed_dict={observations: np.identity(16)[s:s+1], nextQ:targetQ})\n",
    "            reward_all += reward\n",
    "            s = s1\n",
    "            if done == True:\n",
    "                epsilon = 1./((i/50) + 10)\n",
    "                break\n",
    "          \n",
    "        j_list.append(j)\n",
    "        r_list.append(reward_all)\n",
    "        \n",
    "print(\"Percent of succesful episodes: {} %\".format((sum(r_list)/num_episodes)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(j_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To learn more\n",
    "\n",
    "[My In-progress Repo](https://github.com/Robotikus/openAI-reinforcement)\n",
    "\n",
    "[Denny Britz Repo](https://github.com/dennybritz/reinforcement-learning)\n",
    "\n",
    "[Deep RL Bootcamp](https://sites.google.com/view/deep-rl-bootcamp/lectures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "metadata": {
  "name": "",
  "signature": "sha256:a7552ffa0a84c1e8c1cc8a23381d43570a430819154403f214872a0c182a5bd7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#direct from iPython Notebook\n",
      "\n",
      "#1 If you\u2019ve done excel then you have done functional programming\n",
      "#  Port excel work into python script\n",
      "\n",
      "line1 = range(1, 9)\n",
      "line2 = range(10, 50, 5)\n",
      "#print test to see what we have correlates to excel data\n",
      "print(line1, line2)\n",
      "\n",
      "total = sum(line1) + sum(line2)\n",
      "\n",
      "#have states changed?\n",
      "print(line1, line2)\n",
      "\n",
      "#or\n",
      "print(sum(range(1,9)) + sum(range(10,50,5)))\n",
      "\n",
      "#have states changed?\n",
      "print(line1, line2)\n",
      "\n",
      "#or \n",
      "from itertools import chain\n",
      "print(sum(chain(range(1,9), range(10,50,5))))\n",
      "\n",
      "#have states changed?\n",
      "print(line1, line2)\n",
      " \n",
      "#2 discussion of pros and cons of the above code and keep these principles in mind for the folowing\n",
      "\n",
      "\n",
      "#3 brief overview of generators and comprehensions\n",
      "#  intro to iterators terminology, list comps, and generators\n",
      "iterable = range(20)\n",
      "#i is the iterator\n",
      "for i in iterable:\n",
      "    print(i*2)\n",
      "\n",
      "alpha_iter = \"abcdefghijksmellonop\"\n",
      "for i in alpha_iter:\n",
      "    print(i*2)\n",
      "\n",
      "#but if we want it in a data structure that we can manipulate further\n",
      "def get_double_letters(ltrs):\n",
      "    doubles = list()\n",
      "    for i in alpha_iter:\n",
      "        doubles.append(i*2)\n",
      "    return doubles\n",
      "\n",
      "#list comprehensions\n",
      "double_letters = [ltr*2 for ltr in alpha_iter]\n",
      "\n",
      "#generator function\n",
      "def gen_double_letters(ltrs):\n",
      "    for i in alpha_iter:\n",
      "        yield i*2    \n",
      " \n",
      "#won't work btw\n",
      "#for i in alpha_iter:\n",
      "#    yield i*2        \n",
      "\n",
      "print(get_double_letters(alpha_iter))\n",
      "print(gen_double_letters(alpha_iter))\n",
      "\n",
      "gen_object = gen_double_letters(alpha_iter)\n",
      "\n",
      "gen_object.next()\n",
      "gen_object.next()\n",
      "\n",
      "list(gen_object)\n",
      "\n",
      "#gen expressions! write them backwards at first\n",
      "def gen_dubs(ltrs):\n",
      "    for i in (ltr*2 for ltr in ltrs):\n",
      "        yield i\n",
      "\n",
      "#4 basic appiclation: compare cleaning text methods on larger text corpuses converted into iterables for processing\n",
      "#let's apply this to our sample text \n",
      "\n",
      "#just test that we are in proper directory with proper files\n",
      "with open(\"obama_09.txt\") as fp:\n",
      "    o_09 = fp.read()\n",
      "with open(\"obama_14.txt\") as fp:\n",
      "    o_14 = fp.read()\n",
      "\n",
      "#taken from augemented nltk list and experience\n",
      "stop_words = [' ', \" '\", '!', '\"', '#', '&', \"'\", \"'re\", \"'s\", '(', ')', '*', '+', ',', '-', '--', '.', '...', '/', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '``', 'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'amp', 'an', 'and', 'any', 'are', 'as', 'at', 'b', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'c', 'can', 'd', 'did', 'do', 'does', 'doing', 'don', 'down', 'during', 'e', 'each', 'f', 'few', 'for', 'from', 'further', 'g', 'h', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'l', 'm', 'me', 'more', 'most', 'must', 'my', 'myself', 'n', \"n't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'p', 'q', 'r', 'raquo', 's', 'same', 'she', 'should', 'so', 'some', 'such', 't', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'u', 'under', 'until', 'up', 'v', 'very', 'w', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'x', 'y', 'you', 'your', 'yours', 'yourself', 'yourselves', 'z', '|']\n",
      "support_words = ['about', 'above', 'after', 'again', 'against', 'all', 'am', 'amp', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'did', 'do', 'does', 'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'if', 'in', 'into', 'is', 'it', 'its', 'itself', 'just', 'me', 'more', 'most', 'must', 'my', 'myself', \"n't\", 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'raquo', 'same', 'she', 'should', 'so', 'some', 'such', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'you', 'your', 'yours', 'yourself', 'yourselves']\n",
      "\n",
      "\n",
      "def imp_clean_text(text, remove_words=(stop_words, support_words)):\n",
      "    with open(text) as fp:\n",
      "        full_text = fp.read()\n",
      "    full_text_ls = full_text.split()\n",
      "    clean_text = list()\n",
      "    for word in full_text_ls:\n",
      "        if word not in stop_words and support_words:\n",
      "            clean_text.append(word) \n",
      "    return clean_text\n",
      "\n",
      "#what code will look like without introducing concepts first\n",
      "def func_clean_text(text, remove_words=(stop_words, support_words)):\n",
      "    with open(text) as fp:\n",
      "        full = fp.read()\n",
      "    for i in (word for word in full.split() if word not in stop_words and support_words):\n",
      "        yield i\n",
      "\n",
      "imp_14 = imp_clean_text(\"obama_14.txt\", remove_words=(stop_words, support_words))\n",
      "func_14 = func_clean_text(\"obama_14.txt\", remove_words=(stop_words, support_words))\n",
      "print(len(imp_14))\n",
      "print(len(list(func_14)))\n",
      "\n",
      "#what will this give us knowing what we can remember about generators\n",
      "print(imp_14 == list(func_14))\n",
      "\n",
      "#how about this?\n",
      "print(imp_14 == list(func_clean_text(\"obama_14.txt\", remove_words=(stop_words, support_words))))\n",
      "\n",
      "#use time.timeit method in python interpreter\n",
      "# %timeit magic magic method is only for ipython users\n",
      "#%timeit imp_clean_text(\"obama_09.txt\")\n",
      "#%timeit func_clean_text(\"obama_09.txt\")\n",
      "\n",
      "        \n",
      "        \n",
      "\n",
      "#5 deeper dive application: cleaning and manipulating raw text data into consummable information for end user\n",
      "#quick itertools intro and begin text processing\n",
      "\n",
      "#everything in itertools yields a generator!\n",
      "\n",
      "from itertools import chain\n",
      "def func_clean_text(text, remove_words=(stop_words, support_words)):\n",
      "    with open(text) as fp:\n",
      "        full = fp.read()\n",
      "    for i in (word for word in full.split() if word not in chain(stop_words, support_words)):\n",
      "        yield i\n",
      "\n",
      "\n",
      "#heavier duty gen expressions\n",
      "\n",
      "def gen_tokens(text):\n",
      "    \"\"\" str -> gen \n",
      "    call split on our text when its a string\n",
      "    \"\"\"\n",
      "    with open(text) as fp:\n",
      "        fp = fp.read()\n",
      "        for word in fp.split():\n",
      "            yield word\n",
      "\n",
      "def gen_cleaner_words(text):\n",
      "    \"\"\" str -> str \n",
      "    removes commonly 'attached' punctuation marks such as ',''.''*word*' etc.\n",
      "    note that this function does not remove any words from iterable\n",
      "    \"\"\"\n",
      "    for i in gen_tokens(text):\n",
      "        yield i.translate(None, \"!@#$%^&*().,[]+=-_`~<>?:;\")\n",
      "\n",
      "def gen_lowercase_tokens(text):\n",
      "    \"\"\" str -> gen \n",
      "    maintain generator type for lowercase words, \n",
      "    useful for quick comparisons against certain stop word lists\n",
      "    \"\"\"\n",
      "    for word in gen_tokens(text):\n",
      "        yield word.lower()\n",
      "\n",
      "def gen_ns_words(text):\n",
      "    \"\"\" str -> gen \n",
      "    need non stop words before non support words\n",
      "    \"\"\"\n",
      "    for i in (word for word in gen_cleaner_words(text) if word.lower() not in stop_words):\n",
      "        yield i\n",
      "\n",
      "def gen_nsup_words_only(text):\n",
      "    \"\"\"str -> gen\n",
      "    generate words not in support words, mostly a utility function\n",
      "    \"\"\"\n",
      "    for i in (word for word in gen_tokens(text) if word.lower() not in support_words):\n",
      "        yield i\n",
      "        \n",
      "#note for this module's purpose the default type for stop_words will be lowercase\n",
      "def gen_ns_and_nsup_words(text):\n",
      "    \"\"\" str -> gen \n",
      "    generate words not in stop or support words list \n",
      "    \"\"\"\n",
      "    for i in (word for word in gen_ns_words(text) if word.lower() not in support_words):\n",
      "        yield i\n",
      "\n",
      "#Analysis functions\n",
      "def get_text_len_and_content_len(text):\n",
      "    \"\"\" str -> float, float\n",
      "    return length of text list with stop_words omitted and length of 'content'\n",
      "    defined as non 'support words' \"\"\" \n",
      "    num_ns_tokens = float(len(list(gen_ns_words(text))))\n",
      "    num_content_tokens = float(len(list(gen_ns_and_nsup_words(text))))\n",
      "    return (num_ns_tokens, num_content_tokens)\n",
      "\n",
      "def get_content_percentage(text):\n",
      "    \"\"\" str -> float\n",
      "    number of words in text - stop words and support words / number of tokens in text. \n",
      "    Note the difference between this ratio and the lexical diversity ratio, \n",
      "    which only accounts for unique words.\n",
      "    \"\"\"  \n",
      "    try: \n",
      "        result = float(len(list(gen_ns_and_nsup_words(text)))) / float(len(list(gen_tokens(text))))\n",
      "        result = round(result, 2)\n",
      "    except ZeroDivisionError:\n",
      "        pass\n",
      "        result = 0\n",
      "    finally:\n",
      "        return result\n",
      "\n",
      "#print(list(chain(stop_words, support_words)))\n",
      "\n",
      "\n",
      "#test by print statement\n",
      "f1 = func_clean_text(\"obama_14.txt\")\n",
      "#print(list(f1))\n",
      "f2 = gen_ns_and_nsup_words(\"obama_14.txt\")\n",
      "#print(list(f2))\n",
      "\n",
      "#print \"up\" in f1\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "([1, 2, 3, 4, 5, 6, 7, 8], [10, 15, 20, 25, 30, 35, 40, 45])\n",
        "([1, 2, 3, 4, 5, 6, 7, 8], [10, 15, 20, 25, 30, 35, 40, 45])\n",
        "256\n",
        "([1, 2, 3, 4, 5, 6, 7, 8], [10, 15, 20, 25, 30, 35, 40, 45])\n",
        "256\n",
        "([1, 2, 3, 4, 5, 6, 7, 8], [10, 15, 20, 25, 30, 35, 40, 45])\n",
        "0\n",
        "2\n",
        "4\n",
        "6\n",
        "8\n",
        "10\n",
        "12\n",
        "14\n",
        "16\n",
        "18\n",
        "20\n",
        "22\n",
        "24\n",
        "26\n",
        "28\n",
        "30\n",
        "32\n",
        "34\n",
        "36\n",
        "38\n",
        "aa\n",
        "bb\n",
        "cc\n",
        "dd\n",
        "ee\n",
        "ff\n",
        "gg\n",
        "hh\n",
        "ii\n",
        "jj\n",
        "kk\n",
        "ss\n",
        "mm\n",
        "ee\n",
        "ll\n",
        "ll\n",
        "oo\n",
        "nn\n",
        "oo\n",
        "pp\n",
        "['aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg', 'hh', 'ii', 'jj', 'kk', 'ss', 'mm', 'ee', 'll', 'll', 'oo', 'nn', 'oo', 'pp']\n",
        "<generator object gen_double_letters at 0x1056fedc0>\n",
        "4319\n",
        "4319\n",
        "False\n",
        "True"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_text_len_and_content_len(\"obama_09.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "(3200.0, 3200.0)"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(o_09.split())\n",
      "print len(list(gen_ns_words(\"obama_09.txt\")))\n",
      "print len(list(gen_ns_and_nsup_words(\"obama_09.txt\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6260\n",
        "3200\n",
        "3200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_content_percentage(\"obama_09.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "0.51"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_text_len_and_content_len(\"obama_09.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "(3200.0, 3200.0)"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(list(gen_cleaner_words(\"obama_09.txt\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6260\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#test each function, like gen_non-punc_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print support_words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['about', 'above', 'after', 'again', 'against', 'all', 'am', 'amp', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'did', 'do', 'does', 'doing', 'don', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'if', 'in', 'into', 'is', 'it', 'its', 'itself', 'just', 'me', 'more', 'most', 'must', 'my', 'myself', \"n't\", 'no', 'nor', 'not', 'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'raquo', 'same', 'she', 'should', 'so', 'some', 'such', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gen_nsup_words_only(text):\n",
      "    \"\"\"str -> gen\n",
      "    generate words not in support words, mostly a utility function\n",
      "    \"\"\"\n",
      "    for i in (word for word in gen_tokens(text) if word.lower() not in support_words):\n",
      "        yield i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gen_nsup_words_only(\"obama_09.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "<generator object gen_nsup_words_only at 0x1056d58c0>"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(list(gen_nsup_words_only(\"obama_09.txt\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "3502"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(list(gen_ns_words(\"obama_09.txt\")))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "3194"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(list(gen_ns_and_nsup_words(\"obama_09.txt\"))) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "3194"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_text_len_and_content_len(\"obama_09.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "(3194.0, 3194.0)"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ctr_obama_09 = Counter(gen_ns_and_nsup_words(\"obama_09.txt\"))\n",
      "ctr_obama_14 = Counter(gen_ns_and_nsup_words(\"obama_14.txt\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print(ctr_obama_09)\n",
      "ctr_obama_09?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now for a bit more comparative analysis with a basic data type"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "set_obama_09 = set(gen_ns_and_nsup_words(\"obama_09.txt\"))\n",
      "set_obama_14 = set(gen_ns_and_nsup_words(\"obama_14.txt\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(list(gen_tokens(\"obama_09.txt\")))\n",
      "print len(set_obama_09)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "6260\n",
        "1436\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(list(gen_tokens(\"obama_14.txt\")))\n",
      "print len(set_obama_14)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7226\n",
        "1821\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#NLP metric a la George Lakoff\n",
      "def get_rough_keyword_ratio(text):\n",
      "    return float(len(set(gen_ns_and_nsup_words(text)))) / float(len(list(gen_tokens(text)))) * 100"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_rough_keyword_ratio(\"obama_09.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "22.93929712460064"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_rough_keyword_ratio(\"obama_14.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "25.200664267921397"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#a little fun with itertools\n",
      "from itertools import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ifilter(len() > 10, gen_ns_and_nsup_words(\"obama_09.txt\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "len() takes exactly one argument (0 given)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-71-db60b2f69871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mifilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_ns_and_nsup_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"obama_09.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m: len() takes exactly one argument (0 given)"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#ctr_obama_09[:10]  Type Error"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in islice(ctr_obama_09, 10):\n",
      "    print(i)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "personally\n",
        "four\n",
        "earmarks\n",
        "hanging\n",
        "saved\n",
        "worked\n",
        "whose\n",
        "fouryear\n",
        "teaching\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#frozenset(ctr_obama_09)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#funny pairs a la Louis Van Ahn's captcha?\n",
      "#print(list(izip(set_obama_09, set_obama_14)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#well its hard to apply these principles to text, most are meant for numbers\n",
      "#see http://pymotw.com/2/itertools/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-88-e0c5541d846b>, line 2)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-88-e0c5541d846b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    http://pymotw.com/2/itertools/\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}